# Conversation Application
The example application provides a conversational chat interface that supports multiple large language models (LLMs). It allows dynamic selection of the LLM endpoint (dynamic pipeline) to route the user's query to the appropriate model.

# Deploy Conversation Service

The Conversation service can be effortlessly deployed on either Intel Gaudi2 or Intel XEON Scalable Processors.

## Deploy Conversation Service on Gaudi

Refer to the [Gaudi Guide](./deploy/gaudi/README.md) for instructions on deploying Conversation Service on Gaudi.

## Deploy Conversation on Xeon

Refer to the [Xeon Guide](./deploy/xeon/README.md) for instructions on deploying Conversation Service on Xeon.
